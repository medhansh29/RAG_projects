import time
from pymongo import MongoClient
from pymongo.server_api import ServerApi
from langchain_openai import OpenAIEmbeddings
from langchain_community.vectorstores import MongoDBAtlasVectorSearch
from langchain_community.document_loaders import DirectoryLoader
from langchain_openai import ChatOpenAI
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate
import gradio as gr
from gradio.themes.base import Base
import key_param

client = MongoClient(key_param.MONGO_URI, server_api = ServerApi('1'))
dbName = "langchain_demo"
collectionName = "collection_of_text_blobs"
collection = client[dbName][collectionName]

propmt_template = PromptTemplate(
    input_variables= ["context", "question"],
    template = """
Use the following conext to answer the question.
If you don't know the answer based on the context, say "I don't know based on the provided information."

Context:
{context}

Question: {question}
Answer:
"""
)

def query_data(query):
    start_time = time.time()

    docs = vectorStore.similarity_search(query, k=1)
    #as_output = docs[0].page_content

    llm = ChatOpenAI(openai_api_key = key_param.openai_api_key, temperature = 0)
    retriever = vectorStore.as_retriever()
    qa = RetrievalQA.from_chain_type(
        llm=llm, 
        chain_type="stuff", 
        retriever = retriever,
        chain_type_kwargs = {"prompt":propmt_template}
        )
    retriever_output = qa.invoke(query)

    elapsed_time = time.time() - start_time
    time_taken = f"Time Taken: {elapsed_time: .2f} seconds"

    return retriever_output, time_taken


try:
    client.admin.command('ping')
    print("Connected to MongoDB")

    embeddings = OpenAIEmbeddings(openai_api_key = key_param.openai_api_key)

    vectorStore = MongoDBAtlasVectorSearch.from_connection_string(
        connection_string=key_param.MONGO_URI,
        namespace="langchain_demo.collection_of_text_blobs",
        embedding = embeddings
    )

    with gr.Blocks(theme = Base(), title = "Question and Answering App using Vector Search + RAG") as demo:
        gr.Markdown(
            """
            # Question Answering App using Atlas Vector Search + RAG Architecture
            """
        )
        textbox = gr.Textbox(label = "Enter your Question")
        with gr.Row():
            button = gr.Button("Submit", variant="primary")
        with gr.Column():
            #output1 = gr.Textbox(lines = 1, max_lines = 10, label = "Output with just Atlas vector search(returns text fields as it is)")
            output2 = gr.Textbox(lines = 1, max_lines = 10, label = "Output generated by chaining Atlas Vector Search to Langchain's Retriever QA + OpenAI LLM:")
            output3 = gr.Textbox(lines = 1, label = "Execution Time: ")
        button.click(query_data, textbox, outputs = [output2,output3])
    
    demo.launch()

finally:
    client.close()
    print("MongoDB connection closed")



